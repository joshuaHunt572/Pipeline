#!/usr/bin/env python3
"""
Synthesis Module
Generates narrative or long-form output from structured data.
"""

import os
import sys
import json
import time
import logging
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# Add parent directory to path for config access
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    import yaml
    from pydantic import BaseModel, Field
except ImportError:
    print("Missing dependencies. Install: pip install pydantic pyyaml")
    sys.exit(1)


class SynthesisOutput(BaseModel):
    """Schema for Synthesis module output"""
    narrative: str
    executive_summary: str
    recommendations: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class SynthesisModule:
    """Synthesis module - generates narrative and long-form output"""

    def __init__(self, config_path: str = "config/config.yaml"):
        self.config = self._load_config(config_path)
        self.module_config = self.config.get('synthesis', {})

        # Set up paths
        self.inbox_dir = Path(self.module_config.get('inbox', 'pipeline/7_synthesis/Synthesis_Inbox'))
        self.output_dir = Path(self.module_config.get('output', 'pipeline/7_synthesis/Synthesized_Output'))
        self.final_output_dir = Path(self.module_config.get('final_output', 'pipeline/7_synthesis/Final_Output/deliverables'))
        self.archive_dir = self.inbox_dir / 'archive'

        # Create directories
        self.inbox_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.final_output_dir.mkdir(parents=True, exist_ok=True)
        self.archive_dir.mkdir(parents=True, exist_ok=True)

        # Set up logging
        self._setup_logging()

        self.poll_interval = self.module_config.get('poll_interval', 5)
        self.logger.info(f"Synthesis module initialized. Watching: {self.inbox_dir}")

    def _load_config(self, config_path: str) -> Dict:
        """Load YAML configuration"""
        try:
            with open(config_path, 'r') as f:
                return yaml.safe_load(f) or {}
        except FileNotFoundError:
            print(f"Config file not found: {config_path}")
            return {}

    def _setup_logging(self):
        """Configure logging for this module"""
        log_level = self.config.get('logging', {}).get('level', 'INFO')
        log_file = Path(self.module_config.get('log_file', 'pipeline/7_synthesis/synthesis.log'))
        log_file.parent.mkdir(parents=True, exist_ok=True)

        self.logger = logging.getLogger('Synthesis')
        self.logger.setLevel(getattr(logging, log_level))

        # File handler
        fh = logging.FileHandler(log_file)
        fh.setLevel(getattr(logging, log_level))

        # Console handler
        ch = logging.StreamHandler()
        ch.setLevel(getattr(logging, log_level))

        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)

        self.logger.addHandler(fh)
        self.logger.addHandler(ch)

    def generate_narrative(self, data: Dict[str, Any]) -> SynthesisOutput:
        """
        Placeholder for narrative generation logic.
        Replace this with actual LLM-based synthesis.
        """
        self.logger.info("Generating narrative synthesis")

        analysis = data.get('analysis', 'No analysis available')
        actionable = data.get('actionable', [])
        context = data.get('context', {})

        # PLACEHOLDER: Replace with actual narrative generation
        narrative = f"""[PLACEHOLDER NARRATIVE]

OVERVIEW
--------
This document synthesizes the insights from the processing pipeline analysis.

BACKGROUND
----------
{analysis}

KEY FINDINGS
------------
Based on the analysis of the processed data, several important patterns emerged:

1. Total actionable items identified: {len(actionable)}
2. Context information processed: {len(context)} data points

DETAILED ANALYSIS
-----------------
"""

        for i, item in enumerate(actionable, 1):
            action = item.get('action', 'N/A')
            priority = item.get('priority', 'N/A')
            narrative += f"\n{i}. {action}\n   (Priority: {priority})\n"

        narrative += """

CONCLUSION
----------
This narrative would contain a comprehensive synthesis of all findings,
generated by an LLM to provide coherent long-form output.
"""

        executive_summary = f"""[PLACEHOLDER EXECUTIVE SUMMARY]

This executive summary provides a high-level overview of the key findings.

- {len(actionable)} actionable items identified
- Analysis completed with comprehensive context
- Recommendations provided for next steps

This would be expanded by an LLM into a proper executive summary.
"""

        recommendations = [
            "[PLACEHOLDER RECOMMENDATION 1] Address high-priority items first",
            "[PLACEHOLDER RECOMMENDATION 2] Review context for additional insights",
            "[PLACEHOLDER RECOMMENDATION 3] Follow up on pending actions"
        ]

        metadata = {
            "timestamp": datetime.now().isoformat(),
            "processing_module": "synthesis",
            "version": "1.0",
            "narrative_length": len(narrative),
            "recommendations_count": len(recommendations)
        }

        return SynthesisOutput(
            narrative=narrative,
            executive_summary=executive_summary,
            recommendations=recommendations,
            metadata=metadata
        )

    def process_primed_file(self, file_path: Path) -> SynthesisOutput:
        """Process a primed JSON file"""
        self.logger.info(f"Processing primed file: {file_path.name}")

        try:
            with open(file_path, 'r') as f:
                data = json.load(f)

            # Generate narrative synthesis
            output = self.generate_narrative(data)

            # Add source metadata
            output.metadata['source_file'] = file_path.name
            if 'metadata' in data:
                output.metadata['source_metadata'] = data['metadata']

            return output

        except json.JSONDecodeError as e:
            self.logger.error(f"Invalid JSON in {file_path.name}: {e}")
            raise
        except Exception as e:
            self.logger.error(f"Error processing {file_path.name}: {e}")
            raise

    def safe_write_output(self, output_data: SynthesisOutput, source_file: Path):
        """Write output files safely with atomic operation"""
        base_name = source_file.stem.replace('_primed', '')

        # Write JSON to Synthesized_Output
        json_filename = f"{base_name}_synthesized.json"
        json_path = self.output_dir / json_filename
        temp_path = json_path.with_suffix('.tmp')

        try:
            # Write to temp file first
            with open(temp_path, 'w') as f:
                json.dump(output_data.model_dump(), f, indent=2)

            # Atomic rename
            temp_path.rename(json_path)
            self.logger.info(f"JSON output written: {json_path}")

        except Exception as e:
            self.logger.error(f"Failed to write JSON output: {e}")
            if temp_path.exists():
                temp_path.unlink()
            raise

        # Write narrative text to Final_Output/deliverables
        narrative_filename = f"{base_name}_narrative.txt"
        narrative_path = self.final_output_dir / narrative_filename

        try:
            with open(narrative_path, 'w') as f:
                f.write(output_data.narrative)
            self.logger.info(f"Narrative written: {narrative_path}")
        except Exception as e:
            self.logger.error(f"Failed to write narrative: {e}")

        # Write executive summary to Final_Output/deliverables
        summary_filename = f"{base_name}_executive_summary.txt"
        summary_path = self.final_output_dir / summary_filename

        try:
            with open(summary_path, 'w') as f:
                f.write(output_data.executive_summary)
            self.logger.info(f"Executive summary written: {summary_path}")
        except Exception as e:
            self.logger.error(f"Failed to write executive summary: {e}")

        # Write recommendations to Final_Output/deliverables
        recs_filename = f"{base_name}_recommendations.txt"
        recs_path = self.final_output_dir / recs_filename

        try:
            with open(recs_path, 'w') as f:
                f.write("RECOMMENDATIONS\n")
                f.write("=" * 80 + "\n\n")
                for i, rec in enumerate(output_data.recommendations, 1):
                    f.write(f"{i}. {rec}\n\n")
            self.logger.info(f"Recommendations written: {recs_path}")
        except Exception as e:
            self.logger.error(f"Failed to write recommendations: {e}")

    def archive_file(self, file_path: Path):
        """Move processed file to archive"""
        try:
            archive_path = self.archive_dir / file_path.name
            shutil.move(str(file_path), str(archive_path))
            self.logger.info(f"Archived: {file_path.name}")
        except Exception as e:
            self.logger.error(f"Failed to archive {file_path.name}: {e}")

    def process_inbox(self):
        """Process all JSON files in inbox directory"""
        files = [
            f for f in self.inbox_dir.iterdir()
            if f.is_file() and f.suffix == '.json'
        ]

        if not files:
            return

        self.logger.info(f"Found {len(files)} file(s) to process")

        for file_path in files:
            try:
                self.logger.info(f"Processing: {file_path.name}")

                # Process the primed file
                output = self.process_primed_file(file_path)

                # Write output
                self.safe_write_output(output, file_path)

                # Archive the source file
                self.archive_file(file_path)

                self.logger.info(f"Successfully processed: {file_path.name}")

            except Exception as e:
                self.logger.error(f"Error processing {file_path.name}: {e}", exc_info=True)

    def run(self):
        """Main loop - poll inbox and process files"""
        self.logger.info("Starting Synthesis module polling loop")

        try:
            while True:
                self.process_inbox()
                time.sleep(self.poll_interval)

        except KeyboardInterrupt:
            self.logger.info("Synthesis module stopped by user")
        except Exception as e:
            self.logger.error(f"Fatal error in Synthesis module: {e}", exc_info=True)
            raise


def main():
    """Entry point"""
    module = SynthesisModule()
    module.run()


if __name__ == "__main__":
    main()
